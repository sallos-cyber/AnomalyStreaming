{
  "paragraphs": [
    {
      "text": "%spark.conf \nSPARK_HOME /home/sscd/Programme/spark/spark-3.1.1-bin-hadoop3.2\n",
      "user": "anonymous",
      "dateUpdated": "2022-04-04 11:06:06.437",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/text",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1644597367854_142126755",
      "id": "paragraph_1644597367854_142126755",
      "dateCreated": "2022-02-11 17:36:07.855",
      "dateStarted": "2022-04-04 11:06:06.574",
      "dateFinished": "2022-04-04 11:06:06.640",
      "status": "FINISHED"
    },
    {
      "text": "%pyspark\nprint(spark)",
      "user": "anonymous",
      "dateUpdated": "2022-04-04 11:06:10.104",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "is this widowedCounts streaming?\nTrue\nroot\n |-- window: struct (nullable \u003d false)\n |    |-- start: timestamp (nullable \u003d true)\n |    |-- end: timestamp (nullable \u003d true)\n |-- count: long (nullable \u003d false)\n\nNone\nroot\n |-- window: struct (nullable \u003d false)\n |    |-- start: timestamp (nullable \u003d true)\n |    |-- end: timestamp (nullable \u003d true)\n |-- count: long (nullable \u003d false)\n\nNone\nroot\n |-- window: struct (nullable \u003d false)\n |    |-- start: timestamp (nullable \u003d true)\n |    |-- end: timestamp (nullable \u003d true)\n |-- count: long (nullable \u003d false)\n\nNone\nis this widowedCounts streaming?\nTrue\nFalse"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1649059786595_1432023931",
      "id": "paragraph_1649059786595_1432023931",
      "dateCreated": "2022-04-04 10:09:46.747",
      "dateStarted": "2022-04-04 11:06:10.129",
      "dateFinished": "2022-04-04 11:09:17.080",
      "status": "FINISHED"
    },
    {
      "title": "Read in csv, slice into portions of 1,2 or 3 rows and create new csv from that. Write this file to output target. This is to simulate streaming.",
      "text": "%spark.ipyspark\nimport pandas as pd\nimport random\nimport time\n\ninput_file \u003d \"file:///home/sscd/Code/StreamingTest/infostealer_flows_processed.csv\"\noutput_file \u003d\u0027/home/sscd/Programme/zeppelin-0.10.0-bin-netinst/bin/tmp/\u0027\nportion_size\u003d4\n\ndata \u003d pd.read_csv(input_file)\n#enforce certain column ordering:\n\ndf_data \u003d data[[\u0027Datefirstseen\u0027, \u0027SrcIPAddr\u0027, \u0027DstIPAddr\u0027, \u0027Datefirstseenunix\u0027, \u0027Duration\u0027,\u0027Proto\u0027, \u0027SrcPt\u0027, \u0027DstPt\u0027,\u0027Packets\u0027,\u0027Bytes\u0027]]\n\nnr_rows \u003d data.shape[0]\nstart_idx\u003d0\nend_idx\u003drandom.choice(range(1,portion_size))\n\n",
      "user": "anonymous",
      "dateUpdated": "2022-04-04 10:13:46.500",
      "progress": 0,
      "config": {
        "lineNumbers": true,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1645187757864_953935774",
      "id": "paragraph_1645187757864_953935774",
      "dateCreated": "2022-02-18 13:35:57.864",
      "dateStarted": "2022-04-04 10:13:46.519",
      "dateFinished": "2022-04-04 10:13:52.434",
      "status": "FINISHED"
    },
    {
      "text": "%spark.ipyspark\n#for debugging only create a few files\nprint(nr_rows)\n#nr_rows \u003d 30",
      "user": "anonymous",
      "dateUpdated": "2022-04-04 10:14:41.808",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "872\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1645445523646_103850297",
      "id": "paragraph_1645445523646_103850297",
      "dateCreated": "2022-02-21 13:12:03.646",
      "dateStarted": "2022-04-04 10:14:41.844",
      "dateFinished": "2022-04-04 10:14:42.204",
      "status": "FINISHED"
    },
    {
      "text": "%spark.ipyspark\nwhile end_idx \u003c nr_rows:\n    data_subset \u003d data.iloc[start_idx:end_idx]\n    print(data_subset.to_string())\n    file_name_output\u003doutput_file+str(start_idx)+\u0027_\u0027+str(end_idx-1)+\u0027.csv\u0027\n    print(file_name_output)\n    data_subset.to_csv(file_name_output, header\u003dFalse)\n    start_idx\u003dend_idx\n    end_idx \u003d end_idx + random.choice(range(1,portion_size))\n    #time.sleep(5)\n",
      "user": "anonymous",
      "dateUpdated": "2022-02-28 12:13:52.952",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1645445113048_1166752248",
      "id": "paragraph_1645445113048_1166752248",
      "dateCreated": "2022-02-21 13:05:13.048",
      "dateStarted": "2022-02-28 12:13:52.989",
      "dateFinished": "2022-02-28 12:14:01.111",
      "status": "ERROR"
    },
    {
      "text": "%sh\nls -a\n",
      "user": "anonymous",
      "dateUpdated": "2022-02-15 13:14:53.280",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/sh",
        "fontSize": 9.0,
        "results": {},
        "enabled": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1644598666326_2037887308",
      "id": "paragraph_1644598666326_2037887308",
      "dateCreated": "2022-02-11 17:57:46.326",
      "dateStarted": "2022-02-15 13:14:53.323",
      "dateFinished": "2022-02-15 13:15:01.029",
      "status": "FINISHED"
    },
    {
      "text": "%spark.pyspark\nspark\n",
      "user": "anonymous",
      "dateUpdated": "2022-03-07 20:09:18.390",
      "progress": 0,
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\n            \u003cdiv\u003e\n                \u003cp\u003e\u003cb\u003eSparkSession - in-memory\u003c/b\u003e\u003c/p\u003e\n                \n        \u003cdiv\u003e\n            \u003cp\u003e\u003cb\u003eSparkContext\u003c/b\u003e\u003c/p\u003e\n\n            \u003cp\u003e\u003ca href\u003d\"http://10.0.2.15:4040\"\u003eSpark UI\u003c/a\u003e\u003c/p\u003e\n\n            \u003cdl\u003e\n              \u003cdt\u003eVersion\u003c/dt\u003e\n                \u003cdd\u003e\u003ccode\u003ev3.1.1\u003c/code\u003e\u003c/dd\u003e\n              \u003cdt\u003eMaster\u003c/dt\u003e\n                \u003cdd\u003e\u003ccode\u003elocal[*]\u003c/code\u003e\u003c/dd\u003e\n              \u003cdt\u003eAppName\u003c/dt\u003e\n                \u003cdd\u003e\u003ccode\u003ezeppelin_pyspark\u003c/code\u003e\u003c/dd\u003e\n            \u003c/dl\u003e\n        \u003c/div\u003e\n        \n            \u003c/div\u003e\n        "
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1644835469779_350180554",
      "id": "paragraph_1644835469779_350180554",
      "dateCreated": "2022-02-14 11:44:29.782",
      "dateStarted": "2022-03-07 20:09:18.440",
      "dateFinished": "2022-03-07 20:10:39.720",
      "status": "FINISHED"
    },
    {
      "title": "File streaming with filter",
      "text": "%spark.ipyspark\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import TimestampType, StringType, StructType, StructField, IntegerType, LongType\nfrom pyspark.sql.functions import col\n\n# Explicitly set schema\nschema \u003d StructType([ \n                      StructField(\"ctr\", IntegerType(), True),\n                      StructField(\"Datefirstseen\", TimestampType(), True),\n                      StructField(\"SrcIPAddr\", StringType(), True),\n                      StructField(\"DstIPAddr\", StringType(), True),\n                      StructField(\"Datefirstseenunix\", IntegerType(), True),\n                      StructField(\"Duration\", IntegerType(), True),\n                      StructField(\"Proto\", StringType(), True),\n                      StructField(\"SrcPt\", IntegerType(), True),\n                      StructField(\"DstPt\", IntegerType(), True),\n                      StructField(\"Packets\", IntegerType(), True),\n                      StructField(\"Bytes\", IntegerType(), True),\n                      ])\n\n# all files in this directory will be read in one at a time, simulating streaming\ninputFolder \u003d \u0027file:///home/sscd/Programme/zeppelin-0.10.0-bin-netinst/bin/tmp/\u0027\n\nflows_streaming \u003d (\n  spark\n    .readStream\n    .schema(schema)\n    .option(\"maxFilesPerTrigger\", 1)\n    .csv(inputFolder)\n)\n\n# filter input for ip\nwindowedCounts \u003d flows_streaming.where(flows_streaming.SrcIPAddr\u003d\u003d\"192.168.56.12\")\n\n# write the filtered result to file \nquery\u003dwindowedCounts.writeStream.format(\"csv\") \\\n    .outputMode(\"append\")\\\n    .option(\"checkpointLocation\",\"file:///home/sscd/Programme/zeppelin-0.10.0-bin-netinst/bin/checkpoint_zep/\")\\\n    .option(\"path\", \"file:///home/sscd/Programme/zeppelin-0.10.0-bin-netinst/bin/csv/\")\\\n    .start()\n\nquery.awaitTermination(timeout\u003d60)\n\n",
      "user": "anonymous",
      "dateUpdated": "2022-04-04 14:50:01.733",
      "progress": 0,
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "False"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1644837782325_709893896",
      "id": "paragraph_1644837782325_709893896",
      "dateCreated": "2022-02-14 12:23:02.325",
      "dateStarted": "2022-04-04 14:50:02.055",
      "dateFinished": "2022-04-04 14:51:06.909",
      "status": "FINISHED"
    },
    {
      "text": "%spark.ipyspark\ndef array_to_string(my_list):\n    return \u0027[\u0027 + \u0027,\u0027.join([str(elem) for elem in my_list]) + \u0027]\u0027\n\n",
      "user": "anonymous",
      "dateUpdated": "2022-03-07 19:56:13.245",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1645788633183_1968588826",
      "id": "paragraph_1645788633183_1968588826",
      "dateCreated": "2022-02-25 12:30:33.190",
      "dateStarted": "2022-03-07 19:56:13.268",
      "dateFinished": "2022-03-07 19:56:13.284",
      "status": "ERROR"
    },
    {
      "title": "File streaming with aggregation",
      "text": "%spark.ipyspark\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import TimestampType, StringType, StructType, StructField, IntegerType, LongType\nfrom pyspark.sql.functions import col\n\n# Explicitly set schema\nschema \u003d StructType([ \n                      StructField(\"ctr\", IntegerType(), True),\n                      StructField(\"Datefirstseen\", TimestampType(), True),\n                      StructField(\"SrcIPAddr\", StringType(), True),\n                      StructField(\"DstIPAddr\", StringType(), True),\n                      StructField(\"Datefirstseenunix\", IntegerType(), True),\n                      StructField(\"Duration\", IntegerType(), True),\n                      StructField(\"Proto\", StringType(), True),\n                      StructField(\"SrcPt\", IntegerType(), True),\n                      StructField(\"DstPt\", IntegerType(), True),\n                      StructField(\"Packets\", IntegerType(), True),\n                      StructField(\"Bytes\", IntegerType(), True),\n                      ])\n\ninputPath \u003d \u0027file:///home/sscd/Programme/zeppelin-0.10.0-bin-netinst/bin/tmp/\u0027\n\n\nflows_streaming \u003d (\n  spark\n    .readStream\n    .schema(schema)\n    .option(\"maxFilesPerTrigger\", 1)\n    .csv(inputPath)\n)\n\n#flows_streaming.printSchema()\n# Group the data by window and word and compute the count of each group\n#windowedCounts \u003d flows_streaming \\\n#    .withWatermark(\"Datefirstseen\", \"10 minutes\") \\\n#    .groupBy(window(\"Datefirstseen\", \"1 minutes\")).count()\n\n\n#windowedCounts \u003d flows_streaming.withWatermark(\"Datefirstseen\", \"10 minutes\").groupBy(window(\"Datefirstseen\", \"5 minutes\")).count()\n#print(windowedCounts.printSchema())\n\n\n#array_to_string_udf \u003d udf(array_to_string,StringType())\n\n#windowedCounts \u003d windowedCounts.withColumn(\u0027windowstringified\u0027,array_to_string_udf(windowedCounts[\"window\"]))\n#windowedCounts \u003dwindowedCounts.drop(\u0027window\u0027)\n#windowedCounts.printSchema()\n#    .groupBy(\n#        window(flows_streaming.Datefirstseen, \"10 minutes\"),\n#        flows_streaming.Datefirstseen) \\\n#    .count()\n\navgSignalDF \u003d flows_streaming.withWatermark(\"Datefirstseen\", \"1 seconds\").groupBy(window(\"Datefirstseen\",\"1 seconds\")).count()\n\n    \nprint(\u0027is this widowedCounts streaming?\u0027)\nprint(avgSignalDF.isStreaming)\nprint(avgSignalDF.printSchema())\n#avgSignalDF \u003d avgSignalDF.drop(\u0027window\u0027)\nprint(avgSignalDF.printSchema())\n\nprint(avgSignalDF.printSchema())\nprint(\u0027is this widowedCounts streaming?\u0027)\nprint(avgSignalDF.isStreaming)\n\n\nquery\u003davgSignalDF.writeStream.format(\"csv\") \\\n    .outputMode(\"append\")\\\n    .option(\"checkpointLocation\",\"file:///home/sscd/Programme/zeppelin-0.10.0-bin-netinst/bin/checkpoint_zep/\")\\\n    .option(\"path\", \"file:///home/sscd/Programme/zeppelin-0.10.0-bin-netinst/bin/csv/\")\\\n    .start()\n\nquery.awaitTermination(timeout\u003d60)\n\n",
      "user": "anonymous",
      "dateUpdated": "2022-04-04 11:09:34.580",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "editorHide": false,
        "title": true,
        "results": {},
        "enabled": true,
        "lineNumbers": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "is this widowedCounts streaming?\nTrue\nroot\n |-- window: struct (nullable \u003d false)\n |    |-- start: timestamp (nullable \u003d true)\n |    |-- end: timestamp (nullable \u003d true)\n |-- count: long (nullable \u003d false)\n\nNone\nroot\n |-- window: struct (nullable \u003d false)\n |    |-- start: timestamp (nullable \u003d true)\n |    |-- end: timestamp (nullable \u003d true)\n |-- count: long (nullable \u003d false)\n\nNone\nroot\n |-- window: struct (nullable \u003d false)\n |    |-- start: timestamp (nullable \u003d true)\n |    |-- end: timestamp (nullable \u003d true)\n |-- count: long (nullable \u003d false)\n\nNone\nis this widowedCounts streaming?\nTrue\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)\n\u001b[0;32m\u003cipython-input-20-8eaf943a38e8\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 72\u001b[0;31m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m~/Programme/spark/spark-3.1.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m\u003c\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"timeout must be a positive integer or float. Got %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 99\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m~/Programme/spark/spark-3.1.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1304\u001b[0;31m         return_value \u003d get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m~/Programme/spark/spark-3.1.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;31mStreamingQueryException\u001b[0m: CSV data source does not support struct\u003cstart:timestamp,end:timestamp\u003e data type.\n\u003d\u003d\u003d Streaming Query \u003d\u003d\u003d\nIdentifier: [id \u003d c16ee471-1561-4e53-89c7-3683ff82f16a, runId \u003d 83213eba-cca5-42b9-b74b-98a2cd72fd2d]\nCurrent Committed Offsets: {}\nCurrent Available Offsets: {FileStreamSource[file:/home/sscd/Programme/zeppelin-0.10.0-bin-netinst/bin/tmp]: {\"logOffset\":0}}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nAggregate [window#99-T1000ms], [window#99-T1000ms AS window#85-T1000ms, count(1) AS count#98L]\n+- Filter isnotnull(Datefirstseen#64-T1000ms)\n   +- Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(Datefirstseen#64-T1000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) as double) \u003d (cast((precisetimestampconversion(Datefirstseen#64-T1000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) THEN (CEIL((cast((precisetimestampconversion(Datefirstseen#64-T1000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(Datefirstseen#64-T1000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 1000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(Datefirstseen#64-T1000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) as double) \u003d (cast((precisetimestampconversion(Datefirstseen#64-T1000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) THEN (CEIL((cast((precisetimestampconversion(Datefirstseen#64-T1000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(Datefirstseen#64-T1000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 1000000) + 0) + 1000000), LongType, TimestampType)) AS window#99-T1000ms, ctr#63, Datefirstseen#64-T1000ms, SrcIPAddr#65, DstIPAddr#66, Datefirstseenunix#67, Duration#68, Proto#69, SrcPt#70, DstPt#71, Packets#72, Bytes#73]\n      +- EventTimeWatermark Datefirstseen#64: timestamp, 1 seconds\n         +- StreamingExecutionRelation FileStreamSource[file:/home/sscd/Programme/zeppelin-0.10.0-bin-netinst/bin/tmp], [ctr#63, Datefirstseen#64, SrcIPAddr#65, DstIPAddr#66, Datefirstseenunix#67, Duration#68, Proto#69, SrcPt#70, DstPt#71, Packets#72, Bytes#73]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1645614935758_514131887",
      "id": "paragraph_1645614935758_514131887",
      "dateCreated": "2022-02-23 12:15:35.758",
      "dateStarted": "2022-04-04 11:09:34.607",
      "dateFinished": "2022-04-04 11:09:40.460",
      "status": "ERROR"
    },
    {
      "text": "%spark.ipyspark\nquery.stop()\n",
      "user": "anonymous",
      "dateUpdated": "2022-04-04 11:09:08.895",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u003cpyspark.sql.session.SparkSession object at 0x7ff97afd2130\u003e\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1645192227097_1164115353",
      "id": "paragraph_1645192227097_1164115353",
      "dateCreated": "2022-02-18 14:50:27.097",
      "dateStarted": "2022-04-04 11:09:08.927",
      "dateFinished": "2022-04-04 11:09:17.497",
      "status": "FINISHED"
    },
    {
      "title": "Cleanup",
      "text": "%sh\n#cleanup \ncd csv\nrm part*.csv\nrm .part-*\nrm -rf _spark_metadata\nls -all\ncd ../checkpoint_zep\nls\nrm -rf commits metdata offsets sources\nrm metadata\nls -all",
      "user": "anonymous",
      "dateUpdated": "2022-04-04 15:48:43.276",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/sh",
        "fontSize": 9.0,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "rm: das Entfernen von \u0027part*.csv\u0027 ist nicht möglich: Datei oder Verzeichnis nicht gefunden\nrm: das Entfernen von \u0027.part-*\u0027 ist nicht möglich: Datei oder Verzeichnis nicht gefunden\ninsgesamt 16\ndrwxrwxr-x 2 sscd sscd 12288 Apr  4 15:16 .\ndrwxr-xr-x 6 sscd sscd  4096 Feb 18 14:15 ..\nstate\nrm: das Entfernen von \u0027metadata\u0027 ist nicht möglich: Datei oder Verzeichnis nicht gefunden\ninsgesamt 12\ndrwxrwxr-x 3 sscd sscd 4096 Apr  4 15:16 .\ndrwxr-xr-x 6 sscd sscd 4096 Feb 18 14:15 ..\ndrwxr-xr-x 3 sscd sscd 4096 Feb 21 12:57 state\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1645191962803_2072503280",
      "id": "paragraph_1645191962803_2072503280",
      "dateCreated": "2022-02-18 14:46:02.803",
      "dateStarted": "2022-04-04 15:48:43.323",
      "dateFinished": "2022-04-04 15:48:43.514",
      "status": "FINISHED"
    },
    {
      "title": "Basic file streaming",
      "text": "%spark.ipyspark\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import TimestampType, StringType, StructType, StructField, IntegerType, LongType\n\n# Explicitly set schema\nschema \u003d StructType([ \n                      StructField(\"ctr\", IntegerType(), True),\n                      StructField(\"Datefirstseen\", TimestampType(), True),\n                      StructField(\"SrcIPAddr\", StringType(), True),\n                      StructField(\"DstIPAddr\", StringType(), True),\n                      StructField(\"Datefirstseenunix\", IntegerType(), True),\n                      StructField(\"Duration\", IntegerType(), True),\n                      StructField(\"Proto\", StringType(), True),\n                      StructField(\"SrcPt\", IntegerType(), True),\n                      StructField(\"DstPt\", IntegerType(), True),\n                      StructField(\"Packets\", IntegerType(), True),\n                      StructField(\"Bytes\", IntegerType(), True),\n                      ])\n\ninputPath \u003d \u0027file:///home/sscd/Programme/zeppelin-0.10.0-bin-netinst/bin/tmp/\u0027\nstreamingDF \u003d (\n  spark\n    .readStream\n    .schema(schema)\n    .option(\"maxFilesPerTrigger\", 1)\n    .csv(inputPath)\n)\n\n\nquery\u003dstreamingDF.writeStream.format(\"csv\") \\\n    .outputMode(\"append\")\\\n    .option(\"checkpointLocation\",\"file:///home/sscd/Programme/zeppelin-0.10.0-bin-netinst/bin/checkpoint_zep/\")\\\n    .option(\"path\", \"file:///home/sscd/Programme/zeppelin-0.10.0-bin-netinst/bin/csv/\")\\\n    .start()\n\n\n\n\nquery.awaitTermination(timeout\u003d60)\n\n",
      "user": "anonymous",
      "dateUpdated": "2022-02-21 12:47:29.329",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "editorHide": false,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1645195625333_1990737154",
      "id": "paragraph_1645195625333_1990737154",
      "dateCreated": "2022-02-18 15:47:05.333",
      "status": "READY"
    },
    {
      "text": "\n#windowedCounts \u003d words.groupBy(\n#    window(words.timestamp, \u002710 seconds\u0027, \u00271 seconds\u0027),\n#    words.word\n#).count().orderBy(\u0027window\u0027)\n\n#print(windowedCounts)\n#query \u003d wordCounts \\\n#    .writeStream \\\n#    .outputMode(\"complete\") \\\n#    .format(\"console\") \\\n#    .start()\n\n#query \u003d wordCounts \\\n#    .writeStream \\\n#    .outputMode(\"complete\") \\\n#    .format(\"memory\") \\\n#    .queryName(\u0027lalala\u0027) \\\n#    .start()\n\n\n#windowedCounts.writeStream.format(\"csv\").outputMode(\"append\").option(\"checkpointLocation\",\"/home/sscd/tmp\").option(\"path\", \"file:///home/sscd/tmp/bla.csv\").start()\n#option(\"checkpointLocation\", \"/tmp/vaquarkhan/checkpoint\"). \n\n#query \u003d (\n#  streaming_ip_src_counts_df\n#    .writeStream\n#    .format(\"memory\")\n#    .queryName(\"counts\")\n#    .outputMode(\"complete\")\n#    .start()\n#)\n\n#words \u003d streamingDF.select(\n#   explode(\n#       split(streamingDF.value, \" \")\n#   ).alias(\"word\")\n#)\n\n# Generate running word count\n#wordCounts \u003d words.groupBy(\"word\").count()\n\n\n#windowedCounts \u003d words.groupBy(\n#    window(words.Datefirstseen, \"10 minutes\", \"5 minutes\"),\n#    words.word\n#).count()\n\n\n#wordCounts.writeStream.format(\"csv\").outputMode(\"append\").option(\"checkpointLocation\",\"/home/sscd/tmp\").option(\"path\", \"file:///home/sscd/tmp/bla.csv\").start()\n#windowedCounts.writeStream.format(\"csv\").outputMode(\"append\").option(\"checkpointLocation\",\"/home/sscd/tmp\").option(\"path\", \"file:///home/sscd/tmp/bla.csv\").start()\n\n\n\n#query \u003d wordCounts \\\n#    .writeStream \\\n#    .outputMode(\"complete\") \\\n#    .format(\"memory\") \\\n#    .queryName(\u0027a_query_name\u0027) \\\n#    .start()\n",
      "user": "anonymous",
      "dateUpdated": "2022-02-18 14:50:25.055",
      "progress": 0,
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1644843434348_2076921749",
      "id": "paragraph_1644843434348_2076921749",
      "dateCreated": "2022-02-14 13:57:14.349",
      "dateStarted": "2022-02-18 14:49:12.060",
      "dateFinished": "2022-02-18 14:49:12.437",
      "status": "FINISHED"
    },
    {
      "text": "%pyspark\n# read from tcp socket\n#lines \u003d spark.readStream \\\n#    .format(\u0027socket\u0027) \\\n#    .option(\u0027host\u0027, \u0027localhost\u0027) \\\n#    .option(\u0027port\u0027, 9999) \\\n#    .option(\u0027includeTimestamp\u0027, \u0027true\u0027) \\\n#    .load()\n\n# read from file\n#lines \u003dspark.readStream \\\n#    .schema(schema) \\\n#    .format(\u0027csv\u0027) \\\n#    .option(\u0027path\u0027, \u0027file:///home/sscd/Programme/zeppelin-0.10.0-bin-netinst/bin/tmp/\u0027) \\\n#    .load()\n\ndisplay(streamingDF)\nprint(\u0027\\n\u0027)\nprint(streamingDF.isStreaming)\n\n#streaming_ip_src_counts_df \u003d (\n#  streamingDF\n#    .groupBy(\n#      streamingDF.SrcIPAddr\n#    )\n#    .count()\n#)",
      "user": "anonymous",
      "dateUpdated": "2022-02-18 12:14:36.176",
      "progress": 0,
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1645182791296_1760496068",
      "id": "paragraph_1645182791296_1760496068",
      "dateCreated": "2022-02-18 12:13:11.296",
      "status": "READY"
    },
    {
      "text": "%pyspark\n\nspark.table(\"some_name\").show()",
      "user": "anonymous",
      "dateUpdated": "2022-02-14 12:27:42.359",
      "progress": 0,
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "results": {},
        "enabled": true,
        "editorSetting": {}
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1644838058381_202749652",
      "id": "paragraph_1644838058381_202749652",
      "dateCreated": "2022-02-14 12:27:38.381",
      "dateStarted": "2022-02-14 12:27:42.373",
      "dateFinished": "2022-02-14 12:27:43.901",
      "status": "ERROR"
    },
    {
      "text": "%spark.ipyspark\n\nfrom pyspark.sql.functions import explode\nfrom pyspark.sql.functions import split\nfrom pyspark.streaming import StreamingContext\nfrom pyspark.sql import SparkSession\nimport time\nspark\nsc \u003d spark._sc\ncontent \u003d sc.textFile(\"file:///tack.csv\")\nprint(content)\n\ndata_raw_df \u003dspark.read.csv(\u0027file:///home/sscd/Programme/zeppelin-0.10.0-bin-netinst/bin/tack.csv\u0027,header\u003dTrue)\ndata_raw_df.show()\n#spark \u003d SparkSession.builder \\\n#        .appName(\u0027fuckingStreaming\u0027) \\\n#        .getOrCreate()\n\n#\n\n\n\nlines \u003dspark.readStream \\\n    .format(\u0027text\u0027) \\\n    .option(\u0027path\u0027, \u0027file:///home/sscd/Programme/zeppelin-0.10.0-bin-netinst/bin/tmp/\u0027) \\\n    .load()\n\n\nwords \u003d lines.select(\n        explode(\n            split(lines.value, \u0027 \u0027)\n        ).alias(\u0027word\u0027)\n)\nwordCounts \u003d words.groupBy(\u0027word\u0027).count()\n\nquery \u003d wordCounts.writeStream.format(\"memory\").outputMode(\"complete\").queryName(\"test\").start()\n\n#query \u003d wordCounts.writeStream \\\n#   .outputMode(\"complete\") \\\n#    .format(\"memory\") \\\n#    .queryName(\u0027some_name\u0027) \\\n#    .start()",
      "user": "anonymous",
      "dateUpdated": "2022-02-14 12:43:15.164",
      "progress": 100,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 671.5,
              "optionOpen": false
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://10.0.2.15:4040/jobs/job?id\u003d19"
            },
            {
              "jobUrl": "http://10.0.2.15:4040/jobs/job?id\u003d20"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1644232765793_123617773",
      "id": "paragraph_1644232765793_123617773",
      "dateCreated": "2022-02-07 12:19:25.793",
      "dateStarted": "2022-02-14 12:43:15.168",
      "dateFinished": "2022-02-14 12:43:17.636",
      "status": "ERROR"
    },
    {
      "text": "%pyspark\n\nspark.table(\"some_name\").show()\n",
      "user": "anonymous",
      "dateUpdated": "2022-02-11 18:02:25.785",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1644597596313_906959217",
      "id": "paragraph_1644597596313_906959217",
      "dateCreated": "2022-02-11 17:39:56.313",
      "dateStarted": "2022-02-11 18:02:25.809",
      "dateFinished": "2022-02-11 18:03:52.981",
      "status": "ERROR"
    },
    {
      "text": "%spark.pyspark\nquery \u003d (windowedCounts\n  .writeStream\n  .outputMode(\"complete\")\n  .format(\"memory\")\n  .queryName(\"some_name\")\n  .start())\n",
      "user": "anonymous",
      "dateUpdated": "2022-02-11 17:37:40.572",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1644597453361_1295447311",
      "id": "paragraph_1644597453361_1295447311",
      "dateCreated": "2022-02-11 17:37:33.361",
      "status": "READY"
    },
    {
      "text": "%spark.ipyspark\nspark.table(\"some_name\").show()",
      "user": "anonymous",
      "dateUpdated": "2022-02-11 13:46:52.594",
      "progress": 0,
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "results": {},
        "enabled": true,
        "editorSetting": {}
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1644583594615_1634735367",
      "id": "paragraph_1644583594615_1634735367",
      "dateCreated": "2022-02-11 13:46:34.616",
      "status": "READY"
    },
    {
      "text": "object Main {\n    def main(args: Array[String]): Unit \u003d {\n        println(\"Some text\")\n    }\n}\nMain.main(\u0027asfd\u0027)",
      "user": "anonymous",
      "dateUpdated": "2022-02-07 12:51:57.750",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1644234360349_417299784",
      "id": "paragraph_1644234360349_417299784",
      "dateCreated": "2022-02-07 12:46:00.372",
      "dateStarted": "2022-02-07 12:51:57.766",
      "dateFinished": "2022-02-07 12:51:57.812",
      "status": "ERROR"
    },
    {
      "text": "",
      "user": "anonymous",
      "dateUpdated": "2022-02-07 13:54:09.197",
      "progress": 0,
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1644234368000_527136196",
      "id": "paragraph_1644234368000_527136196",
      "dateCreated": "2022-02-07 12:46:08.000",
      "status": "READY"
    }
  ],
  "name": "streaming_from_file",
  "id": "2GVAZPF2U",
  "defaultInterpreterGroup": "spark",
  "version": "0.10.0",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {}
}